{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTConfig, DistilBertModel, DistilBertConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data is loaded and scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./data/X_train_surge_new.npz')\n",
    "Y_train = pd.read_csv('./data/Y_train_surge.csv')\n",
    "X_test = np.load('./data/X_test_surge_new.npz')\n",
    "\n",
    "# train\n",
    "slp_train = X_train['slp']\n",
    "t_slp_train = X_train['t_slp']\n",
    "\n",
    "t_surge1_input_train = X_train['t_surge1_input']\n",
    "t_surge2_input_train = X_train['t_surge2_input']\n",
    "\n",
    "surge1_input_train = X_train['surge1_input']\n",
    "surge2_input_train = X_train['surge2_input']\n",
    "\n",
    "mean_surge1_input_train = np.mean(surge1_input_train, axis=1)\n",
    "std_surge1_input_train = np.std(surge1_input_train, axis=1)\n",
    "mean_surge2_input_train = np.mean(surge2_input_train, axis=1)\n",
    "std_surge2_input_train = np.std(surge2_input_train, axis=1)\n",
    "\n",
    "scaled_surge1_input_train = (surge1_input_train - mean_surge1_input_train[:,None]) / std_surge1_input_train[:,None]\n",
    "scaled_surge2_input_train = (surge2_input_train - mean_surge2_input_train[:,None]) / std_surge2_input_train[:,None]\n",
    "\n",
    "t_surge1_output_train = X_train['t_surge1_output']\n",
    "t_surge2_output_train = X_train['t_surge2_output']\n",
    "\n",
    "# test\n",
    "slp_test = X_test['slp']\n",
    "t_slp_test = X_test['t_slp']\n",
    "\n",
    "t_surge1_input_test = X_test['t_surge1_input']\n",
    "t_surge2_input_test = X_test['t_surge2_input']\n",
    "\n",
    "surge1_input_test = X_test['surge1_input']\n",
    "surge2_input_test = X_test['surge2_input']\n",
    "\n",
    "mean_surge1_input_test = np.mean(surge1_input_test, axis=1)\n",
    "std_surge1_input_test = np.std(surge1_input_test, axis=1)\n",
    "mean_surge2_input_test = np.mean(surge2_input_test, axis=1)\n",
    "std_surge2_input_test = np.std(surge2_input_test, axis=1)\n",
    "\n",
    "scaled_surge1_input_test = (surge1_input_test - mean_surge1_input_test[:,None]) / std_surge1_input_test[:,None]\n",
    "scaled_surge2_input_test = (surge2_input_test - mean_surge2_input_test[:,None]) / std_surge2_input_test[:,None]\n",
    "\n",
    "t_surge1_output_test = X_test['t_surge1_output']\n",
    "t_surge2_output_test = X_test['t_surge2_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to divide the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_1 = Y_train[['surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4', 'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9']].to_numpy()\n",
    "Y_2 = Y_train[['surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4', 'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to have a series of pressures the same as the surges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressures_same_time_1 = np.empty((*(t_surge1_input_train.shape), 41, 41))\n",
    "for i, time_series in enumerate(t_surge1_input_train):\n",
    "    for j, time in enumerate(time_series):\n",
    "        idx = find_nearest(t_slp_train[i,:].flatten(), time)\n",
    "        pressures_same_time_1[i, j, :, :] = slp_train[i, idx, :, :]\n",
    "\n",
    "pressures_same_time_2 = np.empty((*(t_surge2_input_train.shape), 41, 41))\n",
    "for i, time_series in enumerate(t_surge2_input_train):\n",
    "    for j, time in enumerate(time_series):\n",
    "        idx = find_nearest(t_slp_train[i,:].flatten(), time)\n",
    "        pressures_same_time_2[i, j, :, :] = slp_train[i, idx, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pressures_same_time_1 = np.mean(pressures_same_time_1, axis=(1,2,3))\n",
    "std_pressures_same_time_1 = np.std(pressures_same_time_1, axis=(1,2,3))\n",
    "mean_pressures_same_time_2 = np.mean(pressures_same_time_2, axis=(1,2,3))\n",
    "std_pressures_same_time_2 = np.std(pressures_same_time_2, axis=(1,2,3))\n",
    "\n",
    "scaled_pressures_same_time_1 = (pressures_same_time_1 - mean_pressures_same_time_1[:,None, None, None]) / std_pressures_same_time_1[:,None, None, None]\n",
    "scaled_pressures_same_time_2 = (pressures_same_time_2 - mean_pressures_same_time_2[:,None, None, None]) / std_pressures_same_time_2[:,None, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_rounder(t):\n",
    "    return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour) + timedelta(hours=t.minute//30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_hour(array):\n",
    "    hours_array = np.empty_like(array)\n",
    "    for i, times in enumerate(array):\n",
    "        for j, t in enumerate(times):\n",
    "            if t<0:\n",
    "                tt = (datetime(1970,1,1) + timedelta(seconds=int(t))).timetuple()\n",
    "            else:\n",
    "                tt = (datetime.fromtimestamp(int(t))).timetuple() \n",
    "            hours_array[i][j] = tt.tm_yday * 24 + tt.tm_hour\n",
    "    return hours_array / (366 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_in_year_surge_1_train = time_to_hour(t_surge1_input_train)\n",
    "hours_in_year_surge_2_train = time_to_hour(t_surge2_input_train)\n",
    "hours_in_year_surge_1_test = time_to_hour(t_surge1_input_test)\n",
    "hours_in_year_surge_2_test = time_to_hour(t_surge1_input_test)\n",
    "hours_in_year_surge_1_output_train = time_to_hour(t_surge1_output_train)\n",
    "hours_in_year_surge_2_output_train = time_to_hour(t_surge2_output_train)\n",
    "hours_in_year_slp_train = time_to_hour(t_slp_train)\n",
    "hours_in_year_slp_test = time_to_hour(t_slp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hours_in_year_surge_1_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datalen = len(surge1_input_train)\n",
    "trainlen = int(0.9 * datalen)\n",
    "vallen = datalen - trainlen\n",
    "train_idx, val_idx = torch.utils.data.random_split(np.arange(datalen), [trainlen, vallen])\n",
    "\n",
    "pressure1_train, pressure1_val = pressures_same_time_1[train_idx], pressures_same_time_1[val_idx]\n",
    "surge1_train, surge1_val = surge1_input_train[train_idx], surge1_input_train[val_idx]\n",
    "t_surge1_train, t_surge1_val = hours_in_year_surge_1_train[train_idx], hours_in_year_surge_1_train[val_idx]\n",
    "Y_1_train, Y_1_val = Y_1[train_idx], Y_1[val_idx]\n",
    "\n",
    "train_data = list(zip(pressure1_train, surge1_train, t_surge1_train, Y_1_train))\n",
    "val_data = list(zip(pressure1_val, surge1_val, t_surge1_val, Y_1_val))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nnn = 100\n",
    "pressures_same_time_1_flatten = pressures_same_time_1.reshape(-1, 1, 41, 41)[:nnn]\n",
    "surge1_input_train_flatten = surge1_input_train.reshape(-1, 1)[:nnn]\n",
    "hours_in_year_surge_1_train_flatten = hours_in_year_surge_1_train.reshape(-1, 1)[:nnn]\n",
    "Y_1_flatten = Y_1.reshape(-1, 1)[:nnn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datalen = len(pressures_same_time_1_flatten)\n",
    "trainlen = int(0.9 * datalen)\n",
    "vallen = datalen - trainlen\n",
    "train_idx, val_idx = torch.utils.data.random_split(np.arange(datalen), [trainlen, vallen])\n",
    "\n",
    "pressure1_train, pressure1_val = pressures_same_time_1_flatten[train_idx], pressures_same_time_1_flatten[val_idx]\n",
    "surge1_train, surge1_val = surge1_input_train_flatten[train_idx], surge1_input_train_flatten[val_idx]\n",
    "t_surge1_train, t_surge1_val = hours_in_year_surge_1_train_flatten[train_idx], hours_in_year_surge_1_train_flatten[val_idx]\n",
    "Y_1_train, Y_1_val = Y_1_flatten[train_idx], Y_1_flatten[val_idx]\n",
    "\n",
    "train_data = list(zip(pressure1_train, surge1_train, t_surge1_train, Y_1_train))\n",
    "val_data = list(zip(pressure1_val, surge1_val, t_surge1_val, Y_1_val))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader_flatten = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader_flatten = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.linspace(1, 0.1, 10)[np.newaxis]\n",
    "\n",
    "def benchmark_weighted_losses(output, target):\n",
    "    loss = torch.mean(w * (output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the transformer to compute the features from the pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = PressureEncorder()\n",
    "# device = torch.device('cuda')\n",
    "# model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for x1, x2, x3, y in tqdm(train_dataloader_flatten, total = len(train_dataloader), leave=False):\n",
    "        # x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        x1 = x1.type(torch.FloatTensor)\n",
    "        x2 = x2.type(torch.FloatTensor)\n",
    "        x3 = x3.type(torch.FloatTensor)\n",
    "        y = y.type(torch.FloatTensor)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model((x1, x2, x3))\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, y in tqdm(val_dataloader_flatten, total = len(val_dataloader), leave = False):\n",
    "            # x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "            x1 = x1.type(torch.FloatTensor)\n",
    "            x2 = x2.type(torch.FloatTensor)\n",
    "            x3 = x3.type(torch.FloatTensor)\n",
    "            y = y.type(torch.FloatTensor)\n",
    "            pred = model((x1, x2, x3))\n",
    "            loss = criterion(pred, y)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= (len(val_dataloader)*batch_size)\n",
    "    print(f'Epoch {epoch+1}: Validation Loss = {val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnn = 200\n",
    "\n",
    "pressures_same_time_1_flatten = scaled_pressures_same_time_1[:nnn]\n",
    "pressures_same_time_2_flatten = scaled_pressures_same_time_2[:nnn]\n",
    "\n",
    "surge1_input_train_flatten = surge1_input_train[:nnn]\n",
    "surge2_input_train_flatten = surge2_input_train[:nnn]\n",
    "\n",
    "hours_in_year_surge_1_train_flatten = hours_in_year_surge_1_train[:nnn]\n",
    "hours_in_year_surge_2_train_flatten = hours_in_year_surge_2_train[:nnn]\n",
    "\n",
    "Y_1_flatten = Y_1[:nnn]\n",
    "Y_2_flatten = Y_2[:nnn]\n",
    "\n",
    "mean_pressures_same_time_1_flatten = mean_pressures_same_time_1[:nnn]\n",
    "std_pressures_same_time_1_flatten = std_pressures_same_time_1[:nnn]\n",
    "mean_pressures_same_time_2_flatten = mean_pressures_same_time_2[:nnn]\n",
    "std_pressures_same_time_2_flatten = std_pressures_same_time_2[:nnn]\n",
    "\n",
    "mean_surge1_input_train_flatten = mean_surge1_input_train[:nnn]\n",
    "std_surge1_input_train_flatten = std_surge1_input_train[:nnn]\n",
    "mean_surge2_input_train_flatten = mean_surge2_input_train[:nnn]\n",
    "std_surge2_input_train_flatten = std_surge2_input_train[:nnn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = len(pressures_same_time_1_flatten)\n",
    "trainlen = int(0.9 * datalen)\n",
    "vallen = datalen - trainlen\n",
    "train_idx, val_idx = torch.utils.data.random_split(np.arange(datalen), [trainlen, vallen])\n",
    "\n",
    "pressure1_train, pressure1_val = pressures_same_time_1_flatten[train_idx], pressures_same_time_1_flatten[val_idx]\n",
    "surge1_train, surge1_val = surge1_input_train_flatten[train_idx], surge1_input_train_flatten[val_idx]\n",
    "t_surge1_train, t_surge1_val = hours_in_year_surge_1_train_flatten[train_idx], hours_in_year_surge_1_train_flatten[val_idx]\n",
    "Y_1_train, Y_1_val = Y_1_flatten[train_idx], Y_1_flatten[val_idx]\n",
    "\n",
    "pressure2_train, pressure2_val = pressures_same_time_2_flatten[train_idx], pressures_same_time_2_flatten[val_idx]\n",
    "surge2_train, surge2_val = surge2_input_train_flatten[train_idx], surge2_input_train_flatten[val_idx]\n",
    "t_surge2_train, t_surge2_val = hours_in_year_surge_2_train_flatten[train_idx], hours_in_year_surge_2_train_flatten[val_idx]\n",
    "Y_2_train, Y_2_val = Y_2_flatten[train_idx], Y_2_flatten[val_idx]\n",
    "\n",
    "mean_pressures_same_time_1_train, mean_pressures_same_time_1_val = mean_pressures_same_time_1_flatten[train_idx], mean_pressures_same_time_1_flatten[val_idx]\n",
    "std_pressures_same_time_1_train, std_pressures_same_time_1_val = std_pressures_same_time_1_flatten[train_idx], std_pressures_same_time_1_flatten[val_idx]\n",
    "mean_pressures_same_time_2_train, mean_pressures_same_time_2_val = mean_pressures_same_time_2_flatten[train_idx], mean_pressures_same_time_2_flatten[train_idx]\n",
    "std_pressures_same_time_2_train, std_pressures_same_time_2_val = std_pressures_same_time_2_flatten[train_idx], std_pressures_same_time_2_flatten[val_idx]\n",
    "\n",
    "mean_surge1_input_train_train, mean_surge1_input_train_val = mean_surge1_input_train_flatten[train_idx], mean_surge1_input_train_flatten[val_idx]\n",
    "std_surge1_input_train_train, std_surge1_input_train_val = std_surge1_input_train_flatten[train_idx], std_surge1_input_train_flatten[val_idx]\n",
    "mean_surge2_input_train_train, mean_surge2_input_train_val = mean_surge2_input_train_flatten[train_idx], mean_surge2_input_train_flatten[train_idx]\n",
    "std_surge2_input_train_train, std_surge2_input_train_val = std_surge2_input_train_flatten[train_idx], std_surge2_input_train_flatten[val_idx]\n",
    "\n",
    "train_data = list(zip(\n",
    "    pressure1_train, \n",
    "    pressure2_train, \n",
    "    t_surge1_train, t_surge2_train, \n",
    "    surge1_train, surge2_train, \n",
    "    mean_surge1_input_train_train, mean_surge2_input_train_train, \n",
    "    std_surge1_input_train_train, std_surge2_input_train_train,\n",
    "    mean_pressures_same_time_1_train, mean_pressures_same_time_2_train,\n",
    "    std_pressures_same_time_1_train, std_pressures_same_time_2_train,\n",
    "    Y_1_train,\n",
    "    Y_2_train\n",
    "))\n",
    "val_data = list(zip(\n",
    "    pressure1_val, \n",
    "    pressure2_val, \n",
    "    t_surge1_val, t_surge2_val, \n",
    "    surge1_val, surge2_val, \n",
    "    mean_surge1_input_train_val, mean_surge2_input_train_val, \n",
    "    std_surge1_input_train_val, std_surge2_input_train_val,\n",
    "    mean_pressures_same_time_1_val, mean_pressures_same_time_2_val,\n",
    "    std_pressures_same_time_1_val, std_pressures_same_time_2_val,\n",
    "    Y_1_val,\n",
    "    Y_2_val\n",
    "))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader_small = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader_small = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelseqsimple = models.EncoderSeqVit()\n",
    "\n",
    "epochs = 10\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelseqsimple.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb0ecc1adcf48069023f0636945bb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden shape torch.Size([8, 512])\n",
      "mean shape torch.Size([8])\n",
      "std shape torch.Size([8])\n",
      "hidden shape torch.Size([8, 512])\n",
      "mean shape torch.Size([8])\n",
      "std shape torch.Size([8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zx/p0g4p5xs6wb1cj_1gyp9zhdc0000gn/T/ipykernel_8893/741385414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodelseqsimple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mallatproj/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mallatproj/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    modelseqsimple.train()\n",
    "    for x in tqdm(train_dataloader_small, total = len(train_dataloader_small), leave=False):\n",
    "        y2 = x[-1].type(torch.FloatTensor)\n",
    "        y1 = x[-2].type(torch.FloatTensor) # y = x[-1].to(device).type(torch.FloatTensor)\n",
    "        xx = []\n",
    "        for inp in x[:-2]:\n",
    "            xx.append(inp.type(torch.FloatTensor))\n",
    "        xx = tuple(xx)\n",
    "        optimizer.zero_grad()\n",
    "        pred = modelseqsimple(xx)\n",
    "        y = torch.concat([y1, y2], dim=1)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    modelseqsimple.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, y in tqdm(val_dataloader_small, total = len(val_dataloader_small), leave = False):\n",
    "            # x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "            y2 = x[-1].type(torch.FloatTensor)\n",
    "            y1 = x[-2].type(torch.FloatTensor) # y = x[-1].to(device).type(torch.FloatTensor)\n",
    "            xx = []\n",
    "            for inp in x[:-2]:\n",
    "                xx.append(inp.type(torch.FloatTensor))\n",
    "            xx = tuple(xx)\n",
    "            pred = modelseqsimple(xx)\n",
    "            y = torch.concat([y1, y2], dim=1)\n",
    "            loss = criterion(pred, y)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= (len(val_dataloader_small)*batch_size)\n",
    "    print(f'Epoch {epoch+1}: Validation Loss = {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2,3,4,5,6,7,8,9]\n",
    "a[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1d83f4d4fbbec24dd5335a3ea8411cfa594cd5b986a6b3006b93e5d6545557f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mallatproj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
